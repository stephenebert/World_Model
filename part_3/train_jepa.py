import os
import sys

import torch
import torch.optim as optim

from world_model_jepa import JEPAWorldModel


def get_device():
    """Pick CUDA, MPS (Apple), or CPU."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    # On Macs with Apple Silicon
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def main():
    current_dir = os.path.dirname(os.path.abspath(__file__))

    sys.path.append(current_dir)
    try:
        from minigrid_dataset import make_dataloader
    except ImportError as e:
        raise ImportError(
            "Could not import make_dataloader from minigrid_dataset.py. "
            "Make sure minigrid_dataset.py is in the same folder as train_jepa.py."
        ) from e

    # Path to the dataset generated by collect_minigrid_data.py
    npz_path = os.path.join(current_dir, "data", "minigrid_empty8x8_random_200eps.npz")
    if not os.path.exists(npz_path):
        raise FileNotFoundError(
            f"Could not find dataset at {npz_path}.\n"
            "Make sure you've run collect_minigrid_data.py from this folder."
        )

    # Build DataLoader
    dataloader = make_dataloader(
        npz_path,
        batch_size=64,
        shuffle=True,
        num_workers=0,
    )

    device = get_device()
    print(f"Using device: {device}")

    # JEPA world model
    model = JEPAWorldModel(in_channels=3, latent_dim=128, num_actions=7)
    model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=1e-3)

    num_epochs = 5
    global_step = 0

    for epoch in range(1, num_epochs + 1):
        model.train()
        epoch_loss = 0.0
        num_batches = 0

        for batch_idx, batch in enumerate(dataloader, start=1):
            obs = batch["obs"].to(device)          # (B, 3, 64, 64)
            next_obs = batch["next_obs"].to(device)
            actions = batch["action"].to(device)   # (B,)

            loss, z_pred, z_target = model(obs, next_obs, actions)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Update target encoder with EMA
            model.update_target_encoder(momentum=0.99)

            batch_loss = loss.item()
            epoch_loss += batch_loss
            num_batches += 1
            global_step += 1

            if batch_idx % 100 == 0:
                avg_loss = epoch_loss / num_batches
                print(
                    f"Epoch {epoch:02d} | Batch {batch_idx:04d} "
                    f"| Step {global_step:05d} | Avg Loss {avg_loss:.4f}"
                )

        if num_batches > 0:
            epoch_loss /= num_batches
        print(f"Epoch {epoch:02d} finished | Mean Loss {epoch_loss:.4f}")

    # Save model weights
    save_path = os.path.join(current_dir, "jepa_world_model.pt")
    torch.save(model.state_dict(), save_path)
    print(f"Saved JEPA world model to {save_path}")


if __name__ == "__main__":
    main()
